callbacks:
  model_checkpoint:
    every_n_train_steps: 200
ckpt_path: checkpoints/text2semantic-400m-v0.3-4k.pth
data:
  _target_: fish_speech.datasets.text.TextDataModule
  batch_size: 1
  max_length: ${max_length}
  num_workers: 4
  tokenizer: ${tokenizer}
  train_dataset: ${train_dataset}
  val_dataset: ${val_dataset}
defaults:
- base
- _self_
max_length: 1024
model:
  _target_: fish_speech.models.text2semantic.TextToSemantic
  lr_scheduler:
    _partial_: true
    _target_: torch.optim.lr_scheduler.LambdaLR
    lr_lambda:
      _partial_: true
      _target_: fish_speech.scheduler.get_cosine_schedule_with_warmup_lr_lambda
      final_lr_ratio: 0.1
      num_training_steps: ${trainer.max_steps}
      num_warmup_steps: 100
  model:
    _target_: fish_speech.models.text2semantic.llama.Transformer
    config:
      _target_: fish_speech.models.text2semantic.llama.ModelArgs
      codebook_size: 168
      dim: 1024
      dropout: 0.1
      max_seq_len: 4096
      n_head: 16
      n_layer: 24
      norm_eps: 1e-5
      num_codebooks: 4
      rope_base: 10000
      vocab_size: 36408
  optimizer:
    _partial_: true
    _target_: torch.optim.AdamW
    betas:
    - 0.9
    - 0.95
    eps: 1e-5
    lr: 2.0e-05
    weight_decay: 0.1
project: text2semantic_400m_finetune_spk
resume_weights_only: true
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: checkpoints
train_dataset:
  _target_: fish_speech.datasets.text.AutoAugTextDataset
  max_length: ${max_length}
  tokenizer: ${tokenizer}
trainer:
  accelerator: gpu
  accumulate_grad_batches: 2
  devices: auto
  gradient_clip_algorithm: norm
  gradient_clip_val: 1.0
  limit_val_batches: 10
  max_steps: 1000
  precision: '32'
val_dataset:
  _target_: fish_speech.datasets.text.AutoAugTextDataset
  max_length: ${max_length}
  tokenizer: ${tokenizer}
